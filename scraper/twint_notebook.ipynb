{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Twitter with Twint\n",
    "\n",
    "This notebook uses the Python [twint](https://github.com/twintproject/twint/wiki/Module) package to scrape Twitter. Set the configuration before running the cell (see the Python comments for guidance).\n",
    "\n",
    "<h3 style=\"color:red;\">Important!</h3>\n",
    "\n",
    "Before you begin, make sure that you have installed the development version of twint and nest_asyncio with the following commands:\n",
    "\n",
    "```python\n",
    "pip install --user --upgrade -e git+https://github.com/twintproject/twint.git@origin/master#egg=twint\n",
    "\n",
    "pip install nest_asyncio\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the value to None for any filters you do not wish to use\n",
    "handle             = None # e.g. 'sekleinman'\n",
    "queryterm          = 'humanities' # e.g. 'humanities\n",
    "language           = 'en'\n",
    "limit              = None # e.g. 20\n",
    "location           = None # e.g. 'London'\n",
    "near               = None # e.g. 'London'\n",
    "year               = None # e.g. '2017'\n",
    "since              = '2018-01-1' # e.g. '2017-12-27'\n",
    "until              = '2018-01-31' # e.g. '2017-12-27'\n",
    "output_format      = 'json' # 'json' or 'csv'\n",
    "# The path to a folder where the tweets will be saved.\n",
    "# A file called \"tweets.json\" will be saved in this folder.\n",
    "# If the file already exists, tweets will be appended to existing data.\n",
    "output_path        ='C:/Users/Scott/OneDrive/Mellon/twint/testscrape'\n",
    "\n",
    "# True/False Options\n",
    "verified           = False # Include only tweets by only verified users\n",
    "hide_output        = True # Prevents large outputs from displaying in the console\n",
    "count              = True # Show the total number of tweets fetched\n",
    "stats              = True # Show the tweet stats in the terminal output\n",
    "\n",
    "# twint scrapes a large number of metadata properties.\n",
    "# Specify all properties you wish to include in your output.\n",
    "# A full list of possibile properties is below.\n",
    "# Note that for some reason `location` causes an error, so\n",
    "# it is commented out.\n",
    "output_properties  = [\n",
    "    'id',\n",
    "    'date',\n",
    "    'username',\n",
    "    'place',\n",
    "    'tweet',\n",
    "    'mentions',\n",
    "    'urls',\n",
    "#     'location',\n",
    "    'hashtags',\n",
    "    'link',\n",
    "    'retweets_count',\n",
    "    'likes_count'\n",
    "]\n",
    "\n",
    "# Full list: \n",
    "# 'id', 'conversation_id', 'created_at', 'date', 'time',\n",
    "# 'timezone', 'user_id', 'username', 'name', 'place', 'tweet',\n",
    "# 'mentions', 'urls', 'photos', 'replies_count', 'retweets_count', \n",
    "# 'likes_count', 'location', 'hashtags', 'link', 'retweet',\n",
    "# 'quote_url', 'video'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import datetime\n",
    "import os\n",
    "import twint\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Helper Function\n",
    "def scrape(options):\n",
    "    \"\"\"Add the docstring.\"\"\"   \n",
    "    config = twint.Config()\n",
    "    config.Format = '{date}: {tweet}'\n",
    "    config.Verified = options['verified']\n",
    "    config.Count = options['count']\n",
    "    config.Stats = options['stats']\n",
    "    config.Hide_output = options['hide_output']\n",
    "    if 'handle' in options and options['handle'] is not None:\n",
    "        config.Username = options['handle']\n",
    "    if 'queryterm' in options and options['queryterm'] is not None:\n",
    "        config.Search = options['queryterm']\n",
    "    if 'limit' in options and options['limit'] is not None:\n",
    "        config.Limit = options['limit']\n",
    "    if 'language' in options and options['language'] is not None:\n",
    "        config.Language = options['language']\n",
    "    if 'location' in options and options['location'] is not None:\n",
    "        config.Location = options['location']\n",
    "    if 'near' in options and options['near'] is not None:\n",
    "        config.Near = options['near']\n",
    "    if 'year' in options and options['year'] is not None:\n",
    "        config.Year = options['year']\n",
    "    if 'since' in options and options['since'] is not None:\n",
    "        config.Since = options['since']\n",
    "    if 'until' in options and options['until'] is not None:\n",
    "        config.Until = options['until']\n",
    "    if 'output_path' in options or 'output_format' in options:\n",
    "        try:\n",
    "            if options['output_path'] == None:\n",
    "                assert options['output_format'] == None\n",
    "            if options['output_format'] == None:\n",
    "                assert options['output_path'] == None\n",
    "            config.Output = options['output_path']\n",
    "            if 'output_format' in options and output_format == 'csv':\n",
    "                config.Store_csv = True\n",
    "            else:\n",
    "                config.Store_json = True\n",
    "        except:\n",
    "            print('\\n\\nYou must set both the output_path and output_format options.')\n",
    "    if 'output_properties' in options and len(options['output_properties']) != 23:\n",
    "        config.Custom[\"tweet\"] = options['output_properties']\n",
    "        \n",
    "    twint.run.Search(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    configs = [\n",
    "        'handle', 'queryterm', 'limit', 'location', 'near',\n",
    "        'language', 'year', 'since', 'until', 'verified',\n",
    "        'count', 'stats', 'output_format', 'output_path',\n",
    "        'output_properties', 'hide_output'\n",
    "    ]\n",
    "    options = {}\n",
    "    for item in configs:\n",
    "        options[item] = eval(item)\n",
    "\n",
    "\n",
    "    # User feedback when the process starts\n",
    "    start_time = datetime.datetime.now() \n",
    "    print('\\n\\nProcess started at ' + start_time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    if hide_output == True:\n",
    "        msg = \"\"\"\\n\\n\n",
    "    To check the progress, use your file explorer/finder to navigate\n",
    "    to your output file and watch the file size update. You will\n",
    "    receive a notification here when the scraping is finished. If \n",
    "    you need to stop the process, stop the Jupyter notebook kernel.\n",
    "    If you resume the process, it should pick up where you left off.\"\"\"\n",
    "    #     print('\\n\\nYou may receive a warning \"CRITICAL:root:twint.get:User:\\'NoneType\\' object is not subscriptable\". This is a bug and you should be able to ignore it.\\n\\n')\n",
    "\n",
    "    # Perform the scraping\n",
    "        scrape(options)\n",
    "\n",
    "    # User feedback when the process ends\n",
    "    time_elapsed = datetime.datetime.now() - start_time \n",
    "    print('\\n\\nFinished!')\n",
    "    print('\\n\\nTime elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
    "except:\n",
    "    print('\\n\\nError:')\n",
    "    print('Could not perform the scraping process. Please check your configuration.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Data\n",
    "\n",
    "The cell below loads the downloaded data into a pandas data frame, where it can be manipulated. Note that very large files may take a while to load. To make things easier, only the first ten rows are displayed (but you can modify this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import pandas as pd\n",
    "\n",
    "records = map(json.loads, open(output_path, encoding='utf-8'))\n",
    "df = pd.DataFrame.from_records(records)\n",
    "sf = df[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the Data\n",
    "\n",
    "There are lots of things you can do once you get the data into a pandas dataframe. This is just one example, which moves the \"tweet\" column to the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = sf.columns.values.tolist()\n",
    "cols = ['date', 'tweet',\n",
    " 'hashtags',\n",
    " 'id',\n",
    " 'likes_count',\n",
    " 'link',\n",
    " 'mentions',\n",
    " 'place',\n",
    " 'retweets_count',\n",
    " 'urls',\n",
    " 'username']\n",
    "sf = sf[cols]\n",
    "sf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
