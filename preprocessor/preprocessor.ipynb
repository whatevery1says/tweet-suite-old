{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Preprocessor\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "1. Normalise unicode\n",
    "2. Remove accents\n",
    "3. Strip html tags\n",
    "4. Tokenise with spacy, removing URLs\n",
    "5. Return a list of tokens that do not match the following criteria according to spaCy's parsing:\n",
    "\n",
    "\t- token is an emoji\n",
    "\t- token is not in a list of text emoticons\n",
    "\t- token is a stop word\n",
    "\t- token is a punctuation mark\n",
    "\t- token is a quotation mark\n",
    "\t- token is a space\n",
    "\t- token is like a number\n",
    "\t- token is like a url\n",
    "\t- token starts with `pic.twitter.com`\n",
    "    - token is classified as a MONEY entity\n",
    "    - token is classified as a DATE entity\n",
    "    - token is classified as a TIME entity\n",
    "    - token is classified as a QUANTITY entity\n",
    "\t- token is only one character\n",
    "\t- token is `'s`\n",
    "    \n",
    "6. Remove any tokens that can be parsed as dates by Python's dateutil package.\n",
    "7. Join the tokens into a space-separated string.\n",
    "\n",
    "This still leaves a good deal of garbage, but the results are probably as clean as we can get Twitter data. The Twitter stoplist is the same as the standard WE1S stoplist, but with added contractions and common abbreviations on Twitter (a total of 629 stop words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "json_file   = 'project_data/json/2014-2017_humanities_tweets_deduped.json'\n",
    "output_file = 'project_data/json/2014-2017_humanities_tweets_scrubbed.json'\n",
    "# Use a list or a string file path\n",
    "STOPWORDS   = 'twitter_stoplist.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor ready.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import unicodedata\n",
    "import ujson as json\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse\n",
    "from ftfy import fix_text\n",
    "from spacy.language import Language\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.symbols import ORTH, LEMMA, POS, TAG\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.tokens import Token\n",
    "from spacymoji import Emoji\n",
    "\n",
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "EMOTICONS = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "# Handle lemmatisation exceptions\n",
    "LEMMATIZATION_CASES = {\n",
    "    \"humanities\": [{ORTH: u'humanities', LEMMA: u'humanities', POS: u'NOUN', TAG: u'NNS'}]\n",
    "}\n",
    "for k, v in LEMMATIZATION_CASES.items():\n",
    "    nlp.tokenizer.add_special_case(k, v)\n",
    "    \n",
    "# Import stopwords\n",
    "if isinstance(STOPWORDS, str):\n",
    "    with open(STOPWORDS, 'r') as f:\n",
    "        STOPWORDS = f.read().split('\\n')\n",
    "# if len(STOP_WORDS) is not 0:\n",
    "#     for item in STOP_WORDS:\n",
    "#         STOP_WORDS.remove(item)\n",
    "for item in STOPWORDS:\n",
    "    STOP_WORDS.add(item)\n",
    "    nlp.vocab[item].is_stop = True\n",
    "\n",
    "def skip_ents(doc, skip=['CARDINAL', 'DATE', 'QUANTITY', 'TIME']):\n",
    "    # Match months\n",
    "    months = re.compile(r'(?:jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|jul(?:y)?|aug(?:ust)?|sept(?:ember)?|oct(?:ober)?|nov(?:ember)?|Dec(?:ember)?)')\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for ent in doc.ents:\n",
    "            merge = True\n",
    "            if ent.label_ in skip:\n",
    "                merge = False\n",
    "#             if ent.label_ == 'DATE' and re.match(months, ent.text.lower()):\n",
    "#                 merge = True\n",
    "            if merge == True:\n",
    "                attrs = {\"tag\": ent.root.tag, \"dep\": ent.root.dep, \"ent_type\": ent.label}\n",
    "                retokenizer.merge(ent, attrs=attrs)\n",
    "    return doc\n",
    "\n",
    "# Custom tokeniser for hashtags, @, and urls\n",
    "def create_tokenizer(nlp):\n",
    "    # contains the regex to match all sorts of urls:\n",
    "    from spacy.lang.tokenizer_exceptions import URL_PATTERN\n",
    "\n",
    "    # spacy defaults: when the standard behaviour is required, they\n",
    "    # need to be included when subclassing the tokenizer\n",
    "    prefix_re = spacy.util.compile_prefix_regex(Language.Defaults.prefixes)\n",
    "    infix_re = spacy.util.compile_infix_regex(Language.Defaults.infixes)\n",
    "    suffix_re = spacy.util.compile_suffix_regex(Language.Defaults.suffixes)\n",
    "\n",
    "    # extending the default url regex with regex for hashtags with \"or\" = |\n",
    "    hashtag_pattern = r'''|^(#[\\w_-]+)$'''\n",
    "    url_and_hashtag = URL_PATTERN + hashtag_pattern\n",
    "    url_and_hashtag_re = re.compile(url_and_hashtag)\n",
    "\n",
    "    # set a custom extension to match if token is a hashtag\n",
    "#     hashtag_getter = lambda token: token.text.startswith('#')\n",
    "#     Token.set_extension('is_hashtag', getter=hashtag_getter)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                     suffix_search=suffix_re.search,\n",
    "                     infix_finditer=infix_re.finditer,\n",
    "                     token_match=url_and_hashtag_re.match\n",
    "                     )\n",
    "\n",
    "def remove_accents(text, method='unicode'):\n",
    "    \"\"\"Replace accents with unaccented letters\"\"\"\n",
    "    if method == 'unicode':\n",
    "        return ''.join(\n",
    "            c\n",
    "            for c in unicodedata.normalize('NFKD', text)\n",
    "            if not unicodedata.combining(c)\n",
    "        )\n",
    "    elif method == 'ascii':\n",
    "        return (\n",
    "            unicodedata.normalize('NFKD', text)\n",
    "            .encode('ascii', errors='ignore')\n",
    "            .decode('ascii')\n",
    "        )\n",
    "    else:\n",
    "        msg = '`method` must be either \"unicode\" and \"ascii\", not {}'.format(method)\n",
    "        raise ValueError(msg)\n",
    "        \n",
    "def stripHtmlTags(html):\n",
    "    if html is None:\n",
    "        return None\n",
    "    else:\n",
    "        return ''.join(BeautifulSoup(html).findAll(text=True)) \n",
    "\n",
    "def preprocess(tweet):\n",
    "    tweet = fix_text(tweet, normalization='NFC')\n",
    "    tweet = remove_accents(tweet, method='unicode')\n",
    "    tweet = stripHtmlTags(tweet).strip()\n",
    "    doc = nlp(tweet)\n",
    "    tokens = [token.norm_.strip().replace(' ', '_') for token in doc \n",
    "              if not token._.is_emoji \n",
    "              and token.text not in EMOTICONS\n",
    "              and not token.is_stop \n",
    "              and not token.is_punct \n",
    "              and not token.is_quote\n",
    "              and not token.is_space \n",
    "              and not token.like_num\n",
    "              and not token.like_url\n",
    "              and not token.text.startswith('pic.twitter.com')\n",
    "              and not token.ent_type_ == 'MONEY'\n",
    "              and not token.ent_type_ == 'DATE'\n",
    "              and not token.ent_type_ == 'TIME'\n",
    "              and not token.ent_type_ == 'QUANTITY'\n",
    "              and token.text != \"'s\"\n",
    "              and len(token.text) > 1\n",
    "             ]\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            parse(token, fuzzy_with_tokens=True)\n",
    "        except:\n",
    "            new_tokens.append(token)\n",
    "# #     ents = [(e.text, e.label_) for e in doc.ents]\n",
    "# #     print(ents)\n",
    "    return ' '.join(new_tokens)  \n",
    "\n",
    "# Load the language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer = create_tokenizer(nlp)\n",
    "# Add spacymoji to the pipeline\n",
    "emoji = Emoji(nlp, merge_spans=False)\n",
    "nlp.add_pipe(emoji, first=True)\n",
    "\n",
    "# Add entity skipping to the pipeline\n",
    "nlp.add_pipe(skip_ents, after='ner')\n",
    "\n",
    "print('Preprocessor ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "\n",
    "%%time\n",
    "\n",
    "# Load records into dataframe\n",
    "records = list(map(json.loads, open(json_file, encoding='utf-8')))\n",
    "with open(output_file, 'w') as f:\n",
    "    for row in records:\n",
    "        row['tidy_tweet'] = preprocess(row['tweet'])\n",
    "        row['name'] = row['date'] + row['link'].replace('https://twitter.com/', '__').replace('/', '_')\n",
    "        f.write(json.dumps(row) + '\\n')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "CPU times: user 5h 42min 13s, sys: 3min 37s, total: 5h 45min 50s\n",
      "Wall time: 13h 41min 51s\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "On a data 446 MB data file, the processing time was recorded as:\n",
    "\n",
    "```\n",
    "CPU times: user 5h 42min 13s, sys: 3min 37s, total: 5h 45min 50s\n",
    "Wall time: 13h 41min 51s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
